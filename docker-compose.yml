version: "3.8"

services:
  backend:
    container_name: llm-backend
    build:
      context: ./backend
      dockerfile: Dockerfile
    env_file:
      - .env
    ports:
      - "${BACKEND_PORT}:${BACKEND_PORT}"
    depends_on:
      - ollama
    environment:
      - PORT=${BACKEND_PORT}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
    # restart: unless-stopped

  frontend:
    container_name: llm-frontend
    build:
      context: ./frontend
      dockerfile: Dockerfile
    env_file:
      - .env
    ports:
      - "${VITE_FRONTEND_PORT}:${VITE_FRONTEND_PORT}"
    depends_on:
      - backend
    environment:
      - PORT=${VITE_FRONTEND_PORT}
      - VITE_API_URL=${VITE_API_URL}
    # restart: unless-stopped

  ollama:
    image: ollama/ollama
    container_name: llm-ollama
    ports:
      - "${OLLAMA_PORT}:${OLLAMA_PORT}"
    volumes:
      - ollama_models:/root/.ollama
    # restart: unless-stopped

  # webui:
  #   image: ghcr.io/open-webui/open-webui:main
  #   container_name: open-webui
  #   ports:
  #     - 3000:8080
  #   volumes:
  #     - webui_data:/app/backend/data
  #   environment:
  #     - OLLAMA_BASE_URL=http://ollama:11434 # URL to Ollama API using docker
  #   depends_on:
  #     - ollama
  #   extra_hosts:
  #     - host.docker.internal:host-gateway
  #   # restart: unless-stopped

volumes:
  ollama_models:
  # webui_data:
