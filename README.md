
---

# 🗺️ LLM Maps Assistant

A full-stack AI-powered map assistant that uses a locally hosted LLM (Mistral via Ollama) to recommend places to eat/go, and displays the results on an interactive Google Map.  
Built with **Node.js**, **React**, **Docker**, and **Ollama**.

---

## 🧠 Features

- 🔍 Ask questions like _“Where can I get sushi near Berlin?”_
- 🗣️ Responses generated by a local LLM (Mistral) using [Ollama](https://ollama.com)
- 📍 Search results displayed interactively via the **Google Maps API**
- 🐳 Fully containerized with **Docker Compose**
- 🌐 Frontend built with **React**

---

## 🏗️ Architecture

```
.
├── backend/
│ ├── src/
│ │ ├── config/
│ │ ├── routes/
│ │ ├── services/
│ │ └── index.ts
│ ├── Dockerfile
│ ├── package.json
│ └── tsconfig.json
├── frontend/
│ ├── src/
│ │ └── App.tsx
│ ├── Dockerfile
│ └── package.json
├── docker-compose.yml
└── README.md
```

---

## 🚀 Getting Started

### Prerequisites

- [Google Maps API Key](https://mapsplatform.google.com/lp/maps-apis/)
- [Docker](https://www.docker.com/)
- [Ollama](https://ollama.com)

### 1. Clone the repo

```bash
git clone https://github.com/fidelisyugita/llm-maps-assistant.git
cd llm-maps-assistant
```

### 2. Set up `.env` in `backend/`

```bash
GOOGLE_MAPS_API_KEY=your_api_key_here
OLLAMA_BASE_URL=http://ollama:11434
PORT=4000
```

### 3. Run with Docker Compose

```bash
docker-compose up --build
```

---

## 📦 Backend (Node.js)

### Base URL

```
http://localhost:4000/api/maps
```

### Example Endpoint

#### POST `/search`

```json
{
  "query": "sushi restaurant",
  "location": "Berlin"
}
```

---

## 💻 Frontend (React)

### Available at

```
http://localhost:3000
```

### Features

- Ask for recommendations in plain English
- View results plotted on Google Maps
- Works seamlessly with local LLM

---

## 🔧 Development

### Backend

```bash
cd backend
npm install
npm run dev
```

### Frontend

```bash
cd frontend
npm install
npm run dev
```

---

## 🧠 AI Model

This project uses **Mistral 7B** served via [Ollama](https://ollama.com):

```bash
ollama run mistral
```

---
